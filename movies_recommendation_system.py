# -*- coding: utf-8 -*-
"""movies_recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TqFB-1X1xOmqarsde3CPCL316ELAdnnG

# Import Library
"""

import pandas as pd
import numpy as np
# from google.colab import drive
# drive.mount("/content/drive/")

import matplotlib.pyplot as plt
import seaborn as sns
palette = sns.color_palette("Set2")
import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""# Data Loading"""

# movies = pd.read_csv("/content/drive/MyDrive/submission_akhir_mlt/ml-latest/movies.csv")
# ratings = pd.read_csv("/content/drive/MyDrive/submission_akhir_mlt/ml-latest/ratings.csv")

movies = pd.read_csv("movies.csv")
ratings = pd.read_csv("ratings.csv")

"""# EDA

## Menganalisis Rincian Dataset

Melihat rincian dari dataset movies
"""

movies.info()

"""**Insight**
- Terdapat 3 kolom pada dataset yang terdiri dari:
  - `movieId` : identitas dari film yang memiliki tipe data int64 (numerikal)
  - `title` : judul dari film yang memiliki tipe data object (kategorikal)
  - `genres` : genre dari film yang memiliki tipe data object (kategorikal)
- Semua kolom pada data memiliki jumlah baris yang sama yaitu **9742** baris data.

Melihat rincian dari dataset movies
"""

ratings.info()

"""**Insight**
- Terdapat 4 kolom pada dataset yang terdiri dari:
  - `userId` : identitas dari pengguna yang memiliki tipe data int64 (numerikal)
  - `movieId` : identitas dari film yang memiliki tipe data int64 (numerikal)
  - `rating` : rating atau penilaian pengguna terhadap film yang memiliki tipe data float64 (numerikal)
  - `timestamp` : waktu pengguna memberi rating pada film yang memiliki tipe data int64 (numerikal)
- Semua kolom pada data memiliki jumlah baris yang sama yaitu **100836** baris data.

## Melihat Ringkasan Data

Melihat dataset movies
"""

movies.head()

"""**Insight**
- Kolom movie id berisi data numerik increment berurutan dari angka 1
- Kolom title berisi judul dari film beserta tahun rilisnya
- Kolom genres berisi beberapa genre yang dipisahkan oleh karakter '|' dari film tersebut.
- Data pada kolom genres perlu dibersihkan untuk mengambil genre utamanya saja

Melihat dataset ratings
"""

ratings.head()

"""**Insight**
- Data berurutan dimulai dari user id 1
- Kolom rating berisi rating pengguna yang memiliki tipe data float kemungkinan terdapat rating dengan nilai pecahan atau koma

## Memeriksa Missing Values dan Data Duplikat

Memeriksa dataset movies
"""

null_val = movies.isna().sum()
duplicated_data = movies.duplicated().sum()
print("=============================================")
print(f"Jumlah Missing Values Pada Dataset Movies: \n{null_val}")
print(f"Jumlah Data Duplikat Pada Dataset Movies: {duplicated_data}")
print("=============================================")

"""Memeriksa dataset ratings"""

null_val = ratings.isna().sum()
duplicated_data = ratings.duplicated().sum()
print("=============================================")
print(f"Jumlah Missing Values Pada Dataset Ratings: \n{null_val}")
print(f"Jumlah Data Duplikat Pada Dataset Ratings: {duplicated_data}")
print("=============================================")

"""**Insight**
- Tidak ada nilai yang hilang atau missing values pada dataset movies dan ratings
- Tidak ada data yang duplikat pada dataset movies dan ratings

## Analisis Distribusi Data

Membersihkan data genres dengan mengambil genre utamanya saja
"""

movies['genres'] = movies['genres'].str.split('|').str[0]
movies['genres'].value_counts()

"""Membuat fungsi untuk menampilkan distribusi setiap data"""

def cekDistribusi(dataframe, kolom):
    df = dataframe.copy()
    fitur = df[kolom].value_counts().sort_values(ascending=False).index
    total = len(df)

    plt.figure(figsize=(8, 5))
    ax = sns.countplot(data=df, x=kolom, order=fitur, palette=palette)

    # Menambahkan nilai aktual dan persentase di atas setiap bar
    for p in ax.patches:
        nilai = int(p.get_height())
        # persentase = 100 * nilai / total
        ax.annotate(f'{nilai}',
                    (p.get_x() + p.get_width() / 2, p.get_height()),
                    ha='center', va='bottom', fontsize=8)

    plt.title(f'Distribusi Kolom {kolom}')
    plt.xlabel(kolom)
    plt.ylabel('Jumlah')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

"""Menampilkan distribusi data genres pada dataset movies"""

cekDistribusi(movies, 'genres')

"""Menampilkan distribusi data rating pada dataset ratings"""

cekDistribusi(ratings, 'rating')

"""# Data Preparation

## Data Cleaning

Menangani Missing Values dan Duplicated Data pada Dataset Movies
"""

clean_movies = movies.dropna().drop_duplicates()
clean_movies.info()

"""Menangani Missing Values dan Duplicated Data pada Dataset Ratings"""

clean_ratings = ratings.dropna().drop_duplicates()
clean_ratings.info()

"""## Pemilihan Fitur

### Dataset Movies

Menghapus nilai **(no_genres_listed)** pada kolom genres
"""

clean_movies = clean_movies[(clean_movies['genres'] != '(no genres listed)')]
clean_movies['genres'].unique()

"""### Dataset Ratings

Menghapus fitur atau kolom `timestamp` karena fitur tersebut tidak relevan untuk melakukan pemodelan
"""

clean_ratings = clean_ratings.drop(columns=['timestamp'], axis=1)
clean_ratings.info()

"""Menghapus data film pada dataset `ratings.csv` yang tidak ada di dataset `movies.csv`"""

clean_ratings = clean_ratings[clean_ratings['movieId'].isin(clean_movies['movieId'])]
print(f"Jumlah data rating setelah menghapus film tidak relevan: {len(clean_ratings)}\n")
clean_ratings.info()

"""## Text Processing"""

len(clean_movies['genres'].unique())

clean_movies['genres'].unique()

clean_movies['genres'] = clean_movies['genres'].replace({'Sci-Fi':'Scifi', 'Film-Noir':'Filmnoir'})
print(clean_movies[clean_movies['genres'] == 'Scifi'].head())
print(clean_movies[clean_movies['genres'] == 'Filmnoir'].head())

groupped_ratings = clean_ratings.groupby('movieId')
groupped_ratings

"""## Data Transformation

### TF-IDF Vectorizer
"""

tfidf = TfidfVectorizer()

tfidf.fit(clean_movies['genres'])
tfidf.get_feature_names_out()

genre_tfid_cbf = tfidf.fit_transform(clean_movies['genres'])
genre_tfid_cbf.shape

genre_tfid_cbf.todense()

"""### Cosine Simmilarity"""

genre_cosin_cbf = cosine_similarity(genre_tfid_cbf)
genre_cosin_cbf

genre_cosin_df = pd.DataFrame(genre_cosin_cbf, index=clean_movies['title'], columns=clean_movies['title'])
genre_cosin_df.sample(5, axis=1, random_state=42).sample(5, axis=0, random_state=45)

"""### Data Encoding"""

clean_ratings_encode = clean_ratings.copy()

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = clean_ratings_encode['userId'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah placeID menjadi list tanpa nilai yang sama
movies_ids = clean_ratings_encode['movieId'].unique().tolist()
print('list moviesID: ', movies_ids)

# Melakukan proses encoding placeID
movies_to_movies_encoded = {x: i for i, x in enumerate(movies_ids)}
print('encoded moviesID : ', movies_to_movies_encoded)

# Melakukan proses encoding angka ke placeID
movies_encoded_to_movies = {i: x for i, x in enumerate(movies_ids)}
print('encoded angka ke moviesID: ', movies_encoded_to_movies)

# Mapping userID ke dataframe user
clean_ratings_encode['user'] = clean_ratings_encode['userId'].map(user_to_user_encoded)

# Mapping placeID ke dataframe resto
clean_ratings_encode['movies'] = clean_ratings_encode['movieId'].map(movies_to_movies_encoded)
clean_ratings_encode.head()

# Mendapatkan jumlah user
jumlah_users = len(user_to_user_encoded)
print(jumlah_users)

# Mendapatkan jumlah resto
jumlah_movies = len(movies_encoded_to_movies)
print(jumlah_movies)

# Mengubah rating menjadi nilai float
clean_ratings_encode['rating'] = clean_ratings_encode['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(clean_ratings_encode['rating'])

# Nilai maksimal rating
max_rating = max(clean_ratings_encode['rating'])

print('Number of User: {}, Number of movies: {}, Min Rating: {}, Max Rating: {}'.format(
    jumlah_users, jumlah_movies, min_rating, max_rating
))

"""## Data Splitting

Mengacak data ratings terlebih dahulu agar distribusinya menjadi random
"""

clean_ratings_random = clean_ratings_encode.sample(frac=0.1, random_state=42)
clean_ratings_random

"""Membagi data menjadi variabel X dan y dan menerapkan normalisasi untuk data rating"""

X = clean_ratings_random[['user', 'movies']].values

y = clean_ratings_random['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * clean_ratings_random.shape[0])
x_train, x_val, y_train, y_val = (
    X[:train_indices],
    X[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(X, y)



"""# Modelling

## Content-Based Filtering Model

Membuat fungsi pembuatan sistem rekomendasi berbasis CBF
"""

def movies_recommendations(nama_movies, similarity_data=genre_cosin_df, items=clean_movies[['title', 'genres']], k=5):
    """
    Rekomendasi movies berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama_movies : tipe data string (str)
                Nama movies (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan movies sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_movies].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_movies agar nama movies yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_movies, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Memeriksa data yang akan digunakan untuk menguji sistem rekomendasi"""

clean_movies[clean_movies['title'] == 'Toy Story (1995)']

"""Menguji dan menjalankan sistem rekomendasi"""

movies_recommendations('Toy Story (1995)')

"""Perhitungan performa model sistem rekomendasi CBF:

$$
\text{Precision@5} = \frac{\text{Jumlah film relevan}}{\text{Jumlah film yang direkomendasikan}} = \frac{5}{5} = 1
$$

## Collaborative Filtering Model

### Inisiasi Arsitektur Model RecommenderNet
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movies_embedding = layers.Embedding( # layer embeddings movies
        num_movies,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movies_bias = layers.Embedding(num_movies, 1) # layer embedding movies bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movies_vector = self.movies_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movies_bias = self.movies_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movies = tf.tensordot(user_vector, movies_vector, 2)

    x = dot_user_movies + user_bias + movies_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""### Inisiasi Model"""

model = RecommenderNet(jumlah_users, jumlah_movies, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Proses Training Model"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 30,
    validation_data = (x_val, y_val)
)

"""Menampilkan Learning Curve dari proses Training Model"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Pengujian Model

Melakukan inisiasi data film yang belum pernah dilihat atau dirating oleh pengguna
"""

df = clean_ratings.copy()
movies_df = clean_movies.copy()
movies_df['genres'] = movies_df['genres'].str.split('|').str[0]
user_id = df.userId.sample(1).iloc[0]
movies_viewed_by_user = df[df.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movies_not_viewed = movies_df[~movies_df['movieId'].isin(movies_viewed_by_user.movieId.values)]['movieId']
movies_not_viewed = list(
    set(movies_not_viewed)
    .intersection(set(movies_to_movies_encoded.keys()))
)

movies_not_viewed = [[movies_to_movies_encoded.get(x)] for x in movies_not_viewed]
user_encoder = user_to_user_encoded.get(user_id)
movies_array = np.hstack(
    ([[user_encoder]] * len(movies_not_viewed), movies_not_viewed)
)

"""Membuat fungsi untuk menjalankan model sistem rekomendasi"""

def run_recommendation_system(user_id, top_user_rated=5, n_recommendations=10):
    user_encoder = user_to_user_encoded.get(user_id)
    if user_encoder is None:
        print(f"User ID {user_id} tidak ditemukan.")
        return

    # Membuat array input untuk prediksi
    movies_array = np.hstack(
        ([[user_encoder]] * len(movies_not_viewed), movies_not_viewed)
    )
    n_recommendations = n_recommendations + 3
    # Prediksi rating untuk film yang belum ditonton
    ratings = model.predict(movies_array).flatten()

    # Ambil indeks top-N rekomendasi tertinggi
    top_ratings_indices = ratings.argsort()[-n_recommendations:][::-1]
    recommended_movies_ids = [
        movies_not_viewed[x][0] for x in top_ratings_indices
    ]

    print('Showing recommendations for user:', user_id)
    print('===' * 13)
    print('Movies with high ratings from user')
    print('----' * 8)

    # Ambil top-N film yang diberi rating tinggi oleh user
    top_movies_user = (
        movies_viewed_by_user
        .sort_values(by='rating', ascending=False)
        .head(top_user_rated)
        .movieId.values
    )

    movies_df_rows = movies_df[movies_df['movieId'].isin(top_movies_user)]
    for row in movies_df_rows.itertuples():
        print(row.title, ':', row.genres)

    print('----' * 8)
    print(f'Top {n_recommendations-3} movie recommendations')
    print('----' * 8)

    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movies_ids)]
    for row in recommended_movies.itertuples():
        print(row.title, ':', row.genres)

"""Menjalankan model sistem rekomendasi"""

run_recommendation_system(4, top_user_rated=5, n_recommendations=10)